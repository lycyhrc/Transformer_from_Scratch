{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 4\n",
    "batch_size = 1\n",
    "input_dim = 512\n",
    "d_model = 512\n",
    "x = torch.randn((batch_size, sequence_length, input_dim)) # x : bt_size, seq_len, input_dim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入数据形状为 `(batch_size, sequence_length, input_dim)`。这表示在每个epochs中，有 batch_size 个样本，每个样本是一个长度为 sequence_length 的序列，每个元素的维度为 input_dim。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv_layer = nn.Linear(input_dim, 3 * d_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题：为什么要定义一个全连接层，而且输出维度为什么是 3 * d_model？**\n",
    "\n",
    "答案：因为需要从同一个输入 x 生成三个不同的向量：查询（Query）、键（Key）和值（Value）。这三个向量都通过 v_layer 生成，所以 v_layer 的输出实际上是这三个向量的集合。(一次生成，后续分开)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1536])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv = qkv_layer(x)  # linear只有最会维度进行变化 （*，H_in）---> (*,H_out)\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'qkv distribution')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGxCAYAAABIjE2TAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAq4ElEQVR4nO3de3xU5Z3H8e8YyJBgMpAgGWZJIKbRpXKTYLNELKFAlHItILC4XBQrlEvNAoJIhWBrUpAFVCpWtyVUjNjtGojFCqFgkBewcjFFaQtrDDdDGoQ4w80Ewtk/WEbHhMvghHmSfN6v13m9nOc855zfHMT5+pznnGOzLMsSAACAQW4JdgEAAADfREABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAHqOJvNpilTptz04x48eFA2m03Z2dnetoyMDNlsNr/2c/bsWWVkZOi9997za7uajtW2bVv179/fr/1cS05OjpYuXVrjOpvNpoyMjIAeD8AlBBQAAfPoo49q+/btfm1z9uxZzZ8/3++AciPHuhFXCyjbt2/Xo48+Wus1AA1Ro2AXAKD+aN26tVq3bl2rxzh79qzCw8NvyrGu5V/+5V+CenygPmMEBTDUunXr1LlzZ9ntdsXHx2vRokXXdQnFsiw99dRTaty4sV599VUdP35coaGhevrpp6v1/fvf/y6bzaYXXnjhqvssKSnR8OHDFRERIYfDoREjRqi0tLRav5rq27Rpk1JTUxUdHa2wsDDFxcVp6NChOnv2rA4ePKjbbrtNkjR//nzZbDbZbDaNGzfOZ3979uzRsGHD1Lx5cyUkJFzxWJfl5uaqY8eOatKkiW6//fZq3y87O1s2m00HDx70aX/vvfdks9m8ozmpqalat26dDh065K3t68es6RLPxx9/rEGDBql58+Zq0qSJOnfurJUrV9Z4nDfeeENz5syRy+VSZGSkevfurf3799f4nYCGhhEUwEB//vOfNWjQIHXr1k2rV69WVVWVFi5cqH/84x9X3a6iokLjxo3TunXr9Pbbb+uBBx6QJPXv318rV67U/PnzdcstX/1/yYoVKxQaGqqHHnroivs8d+6cevfurZKSEmVlZemOO+7QunXrNGLEiGt+j4MHD6pfv36677779Nvf/lbNmjXTZ599pnfffVeVlZVq1aqV3n33XT3wwAMaP36893LJ5dBy2ZAhQzRy5EhNnDhRZ86cueoxCwsLlZ6eroyMDDmdTr3++ut6/PHHVVlZqRkzZlyz5q976aWX9Nhjj6moqEi5ubnX7L9//36lpKSoZcuWeuGFFxQdHa1Vq1Zp3Lhx+sc//qGZM2f69H/qqad077336j//8z/l8Xg0a9YsDRgwQH/7298UEhLiV61AvWMBME5ycrLlcrmsc+fOeds8Ho8VFRVlffOvrSRr8uTJ1okTJ6zu3btb//RP/2QVFhb69MnLy7MkWRs2bPC2XbhwwXK5XNbQoUOvWsvy5cstSdbatWt92n/84x9bkqwVK1Z42+bNm+dT3x/+8AdLUrV6vu748eOWJGvevHnV1l3e39y5c6+47uvatGlj2Wy2asfr06ePFRkZaZ05c8ayLMtasWKFJckqLi726bd582ZLkrV582ZvW79+/aw2bdrUWPs36x45cqRlt9utw4cP+/Tr27evFR4ebn3xxRc+x/nhD3/o0+/3v/+9Jcnavn17jccDGhIu8QCGOXPmjHbu3KkhQ4aoSZMm3vaIiAgNGDCgxm2Ki4vVrVs3eTwe7dixQ506dfJZ37dvXzmdTq1YscLbtn79epWUlOiRRx65aj2bN29WRESEBg4c6NM+atSoa36Xzp07KzQ0VI899phWrlypTz/99Jrb1GTo0KHX3feuu+6q9v1HjRolj8ejPXv23NDxr9emTZvUq1cvxcbG+rSPGzdOZ8+erTap95vntGPHjpKkQ4cO1WqdQF1AQAEMU15erosXL8rpdFZbV1ObJH3wwQc6cOCARowYUePE0UaNGmn06NHKzc3VF198IenSPIxWrVrp/vvvv2o9J06cUExMzHXX8nUJCQnauHGjWrZsqcmTJyshIUEJCQl6/vnnr7nt17Vq1eq6+17tvJ04ccKv4/rrxIkTNdbqcrlqPH50dLTPZ7vdLunSZTWgoSOgAIZp3ry5bDZbjZNQa2qTpBEjRujnP/+55syZo1/84hc19nn44Yf15ZdfavXq1SovL1deXp7GjBlzzbkO0dHRNc59uVIt33Tffffp7bffltvt1o4dO9StWzelp6dr9erV17W9JL+erXK183Y5EFwemaqoqPDp9/nnn1/3cWoSHR2tY8eOVWsvKSmRJLVo0eJb7R9oSAgogGGaNm2q733ve3rrrbf05ZdfettPnTqlt99++4rb/exnP9PSpUs1d+5czZ49u9r6du3aKTk5WStWrFBOTo4qKir08MMPX7Oenj176tSpU8rLy/Npz8nJ8eNbSSEhIUpOTtavfvUrSfJebgn0qMG+ffv0l7/8xactJydHERER6tKli6RLD3STpL179/r0++Z3vFzf9dbWq1cvbdq0yRtILvvd736n8PBwbksG/MBdPICBfv7zn+uBBx5Qnz59NH36dFVVVWnBggVq2rSpTp48ecXtHn/8cd1666167LHHdPr0ab3wwgs+ow+PPPKIJkyYoJKSEqWkpOjOO++8Zi1jxozRkiVLNGbMGD377LNKTEzUO++8o/Xr119z25dfflmbNm1Sv379FBcXpy+//FK//e1vJUm9e/eWdGluTZs2bbR27Vr16tVLUVFRatGihTdE+MvlcmngwIHKyMhQq1attGrVKuXn52vBggUKDw+XJN1zzz268847NWPGDF24cEHNmzdXbm6utm7dWm1/HTp00FtvvaXly5crKSlJt9xyi7p27VrjsefNm6c//vGP6tmzp+bOnauoqCi9/vrrWrdunRYuXCiHw3FD3wlokII9SxdAzfLy8qyOHTtaoaGhVlxcnPXLX/6yxjtX9P938XzdG2+8YTVq1Mh6+OGHraqqKm+72+22wsLCLEnWq6++et21HD161Bo6dKh16623WhEREdbQoUOtbdu2XfMunu3bt1s/+tGPrDZt2lh2u92Kjo62evToYeXl5fnsf+PGjdbdd99t2e12S5I1duxYn/0dP368Wk1XuounX79+1h/+8AfrrrvuskJDQ622bdtaixcvrrb9gQMHrLS0NCsyMtK67bbbrKlTp1rr1q2rdhfPyZMnrWHDhlnNmjWzbDabzzFVw91HH330kTVgwADL4XBYoaGhVqdOnXzOkWV9dRfPf/3Xf/m0FxcXVzunQENlsyzLCkoyAuC3jIwMzZ8/X/y1BVDfMQcFAAAYh4ACAACMwyUeAABgHEZQAACAcQgoAADAOAQUAABgnDr5oLaLFy+qpKREERERfj0CGwAABI9lWTp16pRcLpduueXqYyR1MqCUlJRUe1soAACoG44cOVLji02/rk4GlIiICEmXvmBkZGSQqwEAANfD4/EoNjbW+zt+NXUyoFy+rBMZGUlAAQCgjrme6RlMkgUAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjON3QNmyZYsGDBggl8slm82mNWvWXLHvhAkTZLPZtHTpUp/2iooKTZ06VS1atFDTpk01cOBAHT161N9SAABAPeV3QDlz5ow6deqkZcuWXbXfmjVr9D//8z9yuVzV1qWnpys3N1erV6/W1q1bdfr0afXv319VVVX+lgMAAOohvx9137dvX/Xt2/eqfT777DNNmTJF69evV79+/XzWud1u/eY3v9Frr72m3r17S5JWrVql2NhYbdy4Uffff7+/JQEAgHom4HNQLl68qNGjR+uJJ57QXXfdVW397t27df78eaWlpXnbXC6X2rdvr23bttW4z4qKCnk8Hp8FAADUXwEPKAsWLFCjRo3005/+tMb1paWlCg0NVfPmzX3aY2JiVFpaWuM2WVlZcjgc3iU2NjbQZQMAAIMENKDs3r1bzz//vLKzs6/rTYVfZ1nWFbeZPXu23G63dzly5EggygUAAIbyew7K1bz//vsqKytTXFyct62qqkrTp0/X0qVLdfDgQTmdTlVWVqq8vNxnFKWsrEwpKSk17tdut8tutweyVADfUsKihGCXEHRFM4qCXQJQbwV0BGX06NHau3evCgsLvYvL5dITTzyh9evXS5KSkpLUuHFj5efne7c7duyYPv744ysGFAAA0LD4PYJy+vRpffLJJ97PxcXFKiwsVFRUlOLi4hQdHe3Tv3HjxnI6nbrzzjslSQ6HQ+PHj9f06dMVHR2tqKgozZgxQx06dPDe1QMAABo2vwPKrl271LNnT+/nadOmSZLGjh2r7Ozs69rHkiVL1KhRIw0fPlznzp1Tr169lJ2drZCQEH/LAQAA9ZDNsiwr2EX4y+PxyOFwyO12KzIyMtjlAA0Sc1CYgwL4y5/fb97FAwAAjENAAQAAxiGgAAAA4wT0OSgA6i7mlAAwCSMoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4PEkWaOB4giwAEzGCAgAAjENAAQAAxiGgAAAA4xBQAACAcZgkCzQwTIoFUBcwggIAAIxDQAEAAMYhoAAAAOMQUAAAgHGYJAvUc0yKBVAXMYICAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIzDk2QB4AZd6Sm9RTOKbnIlQP3DCAoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHH8DihbtmzRgAED5HK5ZLPZtGbNGu+68+fPa9asWerQoYOaNm0ql8ulMWPGqKSkxGcfFRUVmjp1qlq0aKGmTZtq4MCBOnr06Lf+MgAAoH7wO6CcOXNGnTp10rJly6qtO3v2rPbs2aOnn35ae/bs0VtvvaUDBw5o4MCBPv3S09OVm5ur1atXa+vWrTp9+rT69++vqqqqG/8mAACg3rBZlmXd8MY2m3JzczV48OAr9tm5c6e+973v6dChQ4qLi5Pb7dZtt92m1157TSNGjJAklZSUKDY2Vu+8847uv//+avuoqKhQRUWF97PH41FsbKzcbrciIyNvtHygQbjSw8RQe3hQG1Azj8cjh8NxXb/ftT4Hxe12y2azqVmzZpKk3bt36/z580pLS/P2cblcat++vbZt21bjPrKysuRwOLxLbGxsbZcNAACCqFYDypdffqknn3xSo0aN8ial0tJShYaGqnnz5j59Y2JiVFpaWuN+Zs+eLbfb7V2OHDlSm2UDAIAgq7V38Zw/f14jR47UxYsX9dJLL12zv2VZstlsNa6z2+2y2+2BLhEAABiqVkZQzp8/r+HDh6u4uFj5+fk+15mcTqcqKytVXl7us01ZWZliYmJqoxwAAFDHBDygXA4n//u//6uNGzcqOjraZ31SUpIaN26s/Px8b9uxY8f08ccfKyUlJdDlAACAOsjvSzynT5/WJ5984v1cXFyswsJCRUVFyeVyadiwYdqzZ4/++Mc/qqqqyjuvJCoqSqGhoXI4HBo/frymT5+u6OhoRUVFacaMGerQoYN69+4duG8GAADqLL8Dyq5du9SzZ0/v52nTpkmSxo4dq4yMDOXl5UmSOnfu7LPd5s2blZqaKklasmSJGjVqpOHDh+vcuXPq1auXsrOzFRIScoNfAwAA1Cff6jkoweLPfdRAQ8dzUG4+noMC1Myo56AAQEOTsCiBYAh8SwQUAABgHAIKAAAwTq09qA1AcHGJAUBdxggKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjNMo2AUA+HYSFiUEuwRcweU/m6IZRUGuBKh7GEEBAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGMfvgLJlyxYNGDBALpdLNptNa9as8VlvWZYyMjLkcrkUFham1NRU7du3z6dPRUWFpk6dqhYtWqhp06YaOHCgjh49+q2+CAAAqD/8DihnzpxRp06dtGzZshrXL1y4UIsXL9ayZcu0c+dOOZ1O9enTR6dOnfL2SU9PV25urlavXq2tW7fq9OnT6t+/v6qqqm78mwAAgHqjkb8b9O3bV3379q1xnWVZWrp0qebMmaMhQ4ZIklauXKmYmBjl5ORowoQJcrvd+s1vfqPXXntNvXv3liStWrVKsbGx2rhxo+6///5v8XUAAEB9ENA5KMXFxSotLVVaWpq3zW63q0ePHtq2bZskaffu3Tp//rxPH5fLpfbt23v7fFNFRYU8Ho/PAgAA6q+ABpTS0lJJUkxMjE97TEyMd11paalCQ0PVvHnzK/b5pqysLDkcDu8SGxsbyLIBAIBhauUuHpvN5vPZsqxqbd90tT6zZ8+W2+32LkeOHAlYrQAAwDwBDShOp1OSqo2ElJWVeUdVnE6nKisrVV5efsU+32S32xUZGemzAACA+iugASU+Pl5Op1P5+fnetsrKShUUFCglJUWSlJSUpMaNG/v0OXbsmD7++GNvHwAA0LD5fRfP6dOn9cknn3g/FxcXq7CwUFFRUYqLi1N6eroyMzOVmJioxMREZWZmKjw8XKNGjZIkORwOjR8/XtOnT1d0dLSioqI0Y8YMdejQwXtXDwAAaNj8Dii7du1Sz549vZ+nTZsmSRo7dqyys7M1c+ZMnTt3TpMmTVJ5ebmSk5O1YcMGRUREeLdZsmSJGjVqpOHDh+vcuXPq1auXsrOzFRISEoCvBAAA6jqbZVlWsIvwl8fjkcPhkNvtZj4KGryERQnBLgHXUDSjKNglAEbw5/ebd/EAAADj+H2JB4AZGDmpO775Z8WICnBtjKAAAADjEFAAAIBxCCgAAMA4BBQAAGAcJskCdQSTYgE0JIygAAAA4xBQAACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADj8CRZwHA8QRZAQ8QICgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4xBQAACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMw7t4AEPxDp766/KfbdGMoiBXApiLERQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMbhLh7AMNy9AwCMoAAAAAMRUAAgSBIWJTBiBlwBAQUAABiHgAIAAIxDQAEAAMYJeEC5cOGCfvaznyk+Pl5hYWG6/fbb9cwzz+jixYvePpZlKSMjQy6XS2FhYUpNTdW+ffsCXQoAAKijAh5QFixYoJdfflnLli3T3/72Ny1cuFDPPfecXnzxRW+fhQsXavHixVq2bJl27twpp9OpPn366NSpU4EuBwAA1EEBDyjbt2/XoEGD1K9fP7Vt21bDhg1TWlqadu3aJenS6MnSpUs1Z84cDRkyRO3bt9fKlSt19uxZ5eTkBLocAABQBwU8oHTv3l1//vOfdeDAAUnSX/7yF23dulU//OEPJUnFxcUqLS1VWlqadxu73a4ePXpo27ZtNe6zoqJCHo/HZwEAAPVXwJ8kO2vWLLndbv3zP/+zQkJCVFVVpWeffVb/+q//KkkqLS2VJMXExPhsFxMTo0OHDtW4z6ysLM2fPz/QpQIAAEMFfATlzTff1KpVq5STk6M9e/Zo5cqVWrRokVauXOnTz2az+Xy2LKta22WzZ8+W2+32LkeOHAl02QAAwCABH0F54okn9OSTT2rkyJGSpA4dOujQoUPKysrS2LFj5XQ6JV0aSWnVqpV3u7KysmqjKpfZ7XbZ7fZAlwoAAAwV8BGUs2fP6pZbfHcbEhLivc04Pj5eTqdT+fn53vWVlZUqKChQSkpKoMsBAAB1UMBHUAYMGKBnn31WcXFxuuuuu/Thhx9q8eLFeuSRRyRdurSTnp6uzMxMJSYmKjExUZmZmQoPD9eoUaMCXQ4AAKiDAh5QXnzxRT399NOaNGmSysrK5HK5NGHCBM2dO9fbZ+bMmTp37pwmTZqk8vJyJScna8OGDYqIiAh0OQAAoA6yWZZlBbsIf3k8HjkcDrndbkVGRga7HCCgeLttw1M0oyjYJQA3hT+/3wEfQQEA+OdaoZQAg4aIlwUCAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4zQKdgEALklYlBDsEmCob/67UTSjKEiVADcPIygAAMA4BBQAAGAcLvEAQcIlHQC4MkZQAACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAUMckLErgScSo9wgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADj1EpA+eyzz/Rv//Zvio6OVnh4uDp37qzdu3d711uWpYyMDLlcLoWFhSk1NVX79u2rjVIAAEAdFPCAUl5ernvvvVeNGzfWn/70J/31r3/Vf/zHf6hZs2bePgsXLtTixYu1bNky7dy5U06nU3369NGpU6cCXQ4AAKiDGgV6hwsWLFBsbKxWrFjhbWvbtq33ny3L0tKlSzVnzhwNGTJEkrRy5UrFxMQoJydHEyZMCHRJAACgjgn4CEpeXp66du2qBx98UC1bttTdd9+tV1991bu+uLhYpaWlSktL87bZ7Xb16NFD27Ztq3GfFRUV8ng8PgsAAKi/Ah5QPv30Uy1fvlyJiYlav369Jk6cqJ/+9Kf63e9+J0kqLS2VJMXExPhsFxMT4133TVlZWXI4HN4lNjY20GUDAACDBDygXLx4UV26dFFmZqbuvvtuTZgwQT/+8Y+1fPlyn342m83ns2VZ1doumz17ttxut3c5cuRIoMsGAAAGCXhAadWqlb773e/6tLVr106HDx+WJDmdTkmqNlpSVlZWbVTlMrvdrsjISJ8FAADUXwEPKPfee6/279/v03bgwAG1adNGkhQfHy+n06n8/Hzv+srKShUUFCglJSXQ5QDGSViUoIRFCcEuAwCMFvC7eP793/9dKSkpyszM1PDhw/XBBx/olVde0SuvvCLp0qWd9PR0ZWZmKjExUYmJicrMzFR4eLhGjRoV6HIAAEAdFPCAcs899yg3N1ezZ8/WM888o/j4eC1dulQPPfSQt8/MmTN17tw5TZo0SeXl5UpOTtaGDRsUERER6HIAAEAdZLMsywp2Ef7yeDxyOBxyu93MR0Gdw+UdBErRjKJglwD4xZ/fb97FAwAAjENAAQAAxiGgAAAA4xBQAACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjBPxlgQAu4Z07AHDjGEEBAADGIaAAAADjEFAAAIBxCCgAAMA4TJIFgDrqWhOxi2YU3aRKgMBjBAUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4/CyQCBArvXiNgDA9WMEBQAAGIeAAgAAjENAAQAAxiGgAAAA4zBJFviWmBwLAIHHCAoAADAOAQUA6qmERQmM8KHOIqAAAADjEFAAAIBxCCgAAMA4tR5QsrKyZLPZlJ6e7m2zLEsZGRlyuVwKCwtTamqq9u3bV9ulAACAOqJWbzPeuXOnXnnlFXXs2NGnfeHChVq8eLGys7N1xx136Be/+IX69Omj/fv3KyIiojZLAoAG55sTZYtmFAWpEuD61doIyunTp/XQQw/p1VdfVfPmzb3tlmVp6dKlmjNnjoYMGaL27dtr5cqVOnv2rHJycmqrHAAAUIfUWkCZPHmy+vXrp969e/u0FxcXq7S0VGlpad42u92uHj16aNu2bTXuq6KiQh6Px2cBAAD1V61c4lm9erX27NmjnTt3VltXWloqSYqJifFpj4mJ0aFDh2rcX1ZWlubPnx/4QgEAgJECPoJy5MgRPf7441q1apWaNGlyxX42m83ns2VZ1doumz17ttxut3c5cuRIQGsGAABmCfgIyu7du1VWVqakpCRvW1VVlbZs2aJly5Zp//79ki6NpLRq1crbp6ysrNqoymV2u112uz3QpQIAAEMFfASlV69e+uijj1RYWOhdunbtqoceekiFhYW6/fbb5XQ6lZ+f792msrJSBQUFSklJCXQ5AACgDgr4CEpERITat2/v09a0aVNFR0d729PT05WZmanExEQlJiYqMzNT4eHhGjVqVKDLAQAAdVCtPgflSmbOnKlz585p0qRJKi8vV3JysjZs2MAzUFAn8PI11HVX+neY56PAJDbLsqxgF+Evj8cjh8Mht9utyMjIYJeDBoaAgvqKgILa5s/vN+/iAQAAxiGgAAAA4xBQAACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxgvKyQKAu4h08AHDzMIICAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcbjMGroHbiwHg5mMEBQAAGIeAAgAAjENAAQAAxiGgAAAA4zBJFvh/TIYFAHMwggIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA7PQQEASLr+ZwEVzSiq5UoARlAAAICBCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMEPKBkZWXpnnvuUUREhFq2bKnBgwdr//79Pn0sy1JGRoZcLpfCwsKUmpqqffv2BboUAABQRwU8oBQUFGjy5MnasWOH8vPzdeHCBaWlpenMmTPePgsXLtTixYu1bNky7dy5U06nU3369NGpU6cCXQ4AAKiDbJZlWbV5gOPHj6tly5YqKCjQ97//fVmWJZfLpfT0dM2aNUuSVFFRoZiYGC1YsEATJky45j49Ho8cDofcbrciIyNrs3w0INf7mG+goeNR97hR/vx+1/ocFLfbLUmKioqSJBUXF6u0tFRpaWnePna7XT169NC2bdtq3EdFRYU8Ho/PAgAA6q9afVmgZVmaNm2aunfvrvbt20uSSktLJUkxMTE+fWNiYnTo0KEa95OVlaX58+fXZqlowBg5AQDz1OoIypQpU7R371698cYb1dbZbDafz5ZlVWu7bPbs2XK73d7lyJEjtVIvAAAwQ62NoEydOlV5eXnasmWLWrdu7W13Op2SLo2ktGrVytteVlZWbVTlMrvdLrvdXlulooFgpAQA6o6Aj6BYlqUpU6borbfe0qZNmxQfH++zPj4+Xk6nU/n5+d62yspKFRQUKCUlJdDlAACAOijgIyiTJ09WTk6O1q5dq4iICO+cE4fDobCwMNlsNqWnpyszM1OJiYlKTExUZmamwsPDNWrUqECXAwAA6qCAB5Tly5dLklJTU33aV6xYoXHjxkmSZs6cqXPnzmnSpEkqLy9XcnKyNmzYoIiIiECXAwAA6qBafw5KbeA5KLgRzEEBAoPnoOBGGfUcFAAAAH8RUAAAgHEIKAAAwDi1+iRZAED9c6X5XMxNQSAxggIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBxuMwYABAS3HyOQGEEBAADGIaAAAADjEFAAALUqYVECbxOH3wgoAADAOEySRb3B/6EBQP3BCAoAADAOAQUAABiHgAIAAIxDQAEAAMZhkizqLCbFAkD9xQgKAAAwDgEFAAAYh0s8AICb4lqXZXmpIL6OERQAAGAcAgoAADAOAQUAABiHgAIAAIzDJFnUOTz/BADqP0ZQAACAcQgoAADAOFzigXG4hAM0TP7+3ee5KfUbIygAAMA4BBQAAGAcAgoAADAOAQUAABiHSbKoNUx2BQDcKEZQAACAcRhBAQDUSbU9SsttzMEV1BGUl156SfHx8WrSpImSkpL0/vvvB7McAABgiKAFlDfffFPp6emaM2eOPvzwQ913333q27evDh8+HKySAACAIWyWZVnBOHBycrK6dOmi5cuXe9vatWunwYMHKysr66rbejweORwOud1uRUZG1nap9Q6TVwHg2+MSkP/8+f0OyhyUyspK7d69W08++aRPe1pamrZt21atf0VFhSoqKryf3W63pEtfFP67+OXFYJcAAHUev0H+u3zOrmdsJCgB5fPPP1dVVZViYmJ82mNiYlRaWlqtf1ZWlubPn1+tPTY2ttZqBADgahxPO4JdQp116tQpORxXP39BvYvHZrP5fLYsq1qbJM2ePVvTpk3zfr548aJOnjyp6OjoGvvXFR6PR7GxsTpy5EiDvlTFefgK5+ISzsNXOBdf4VxcUpfPg2VZOnXqlFwu1zX7BiWgtGjRQiEhIdVGS8rKyqqNqkiS3W6X3W73aWvWrFltlnhTRUZG1rl/yWoD5+ErnItLOA9f4Vx8hXNxSV09D9caObksKHfxhIaGKikpSfn5+T7t+fn5SklJCUZJAADAIEG7xDNt2jSNHj1aXbt2Vbdu3fTKK6/o8OHDmjhxYrBKAgAAhghaQBkxYoROnDihZ555RseOHVP79u31zjvvqE2bNsEq6aaz2+2aN29etctXDQ3n4Suci0s4D1/hXHyFc3FJQzkPQXsOCgAAwJXwskAAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoBhi4MCBiouLU5MmTdSqVSuNHj1aJSUlwS7rpjp48KDGjx+v+Ph4hYWFKSEhQfPmzVNlZWWwSwuKZ599VikpKQoPD69XT06+Hi+99JLi4+PVpEkTJSUl6f333w92STfdli1bNGDAALlcLtlsNq1ZsybYJQVFVlaW7rnnHkVERKhly5YaPHiw9u/fH+yygmL58uXq2LGj9wmy3bp105/+9Kdgl1VrCCiG6Nmzp37/+99r//79+u///m8VFRVp2LBhwS7rpvr73/+uixcv6te//rX27dunJUuW6OWXX9ZTTz0V7NKCorKyUg8++KB+8pOfBLuUm+rNN99Uenq65syZow8//FD33Xef+vbtq8OHDwe7tJvqzJkz6tSpk5YtWxbsUoKqoKBAkydP1o4dO5Sfn68LFy4oLS1NZ86cCXZpN13r1q31y1/+Urt27dKuXbv0gx/8QIMGDdK+ffuCXVqt4DkohsrLy9PgwYNVUVGhxo0bB7ucoHnuuee0fPlyffrpp8EuJWiys7OVnp6uL774Itil3BTJycnq0qWLli9f7m1r166dBg8erKysrCBWFjw2m025ubkaPHhwsEsJuuPHj6tly5YqKCjQ97///WCXE3RRUVF67rnnNH78+GCXEnCMoBjo5MmTev3115WSktKgw4kkud1uRUVFBbsM3CSVlZXavXu30tLSfNrT0tK0bdu2IFUFk7jdbklq8P9dqKqq0urVq3XmzBl169Yt2OXUCgKKQWbNmqWmTZsqOjpahw8f1tq1a4NdUlAVFRXpxRdf5P1MDcjnn3+uqqqqam81j4mJqfb2czQ8lmVp2rRp6t69u9q3bx/scoLio48+0q233iq73a6JEycqNzdX3/3ud4NdVq0goNSijIwM2Wy2qy67du3y9n/iiSf04YcfasOGDQoJCdGYMWNUH67A+XseJKmkpEQPPPCAHnzwQT366KNBqjzwbuRcNEQ2m83ns2VZ1drQ8EyZMkV79+7VG2+8EexSgubOO+9UYWGhduzYoZ/85CcaO3as/vrXvwa7rFoRtJcFNgRTpkzRyJEjr9qnbdu23n9u0aKFWrRooTvuuEPt2rVTbGysduzYUeeH7/w9DyUlJerZs6f3Ldf1ib/noqFp0aKFQkJCqo2WlJWVVRtVQcMydepU5eXlacuWLWrdunWwywma0NBQfec735Ekde3aVTt37tTzzz+vX//610GuLPAIKLXocuC4EZdHTioqKgJZUlD4cx4+++wz9ezZU0lJSVqxYoVuuaV+DfJ9m38nGoLQ0FAlJSUpPz9fP/rRj7zt+fn5GjRoUBArQ7BYlqWpU6cqNzdX7733nuLj44NdklEsy6oXvxM1IaAY4IMPPtAHH3yg7t27q3nz5vr00081d+5cJSQk1PnRE3+UlJQoNTVVcXFxWrRokY4fP+5d53Q6g1hZcBw+fFgnT57U4cOHVVVVpcLCQknSd77zHd16663BLa4WTZs2TaNHj1bXrl29o2iHDx9ucHORTp8+rU8++cT7ubi4WIWFhYqKilJcXFwQK7u5Jk+erJycHK1du1YRERHe0TWHw6GwsLAgV3dzPfXUU+rbt69iY2N16tQprV69Wu+9957efffdYJdWOywE3d69e62ePXtaUVFRlt1ut9q2bWtNnDjROnr0aLBLu6lWrFhhSapxaYjGjh1b47nYvHlzsEurdb/61a+sNm3aWKGhoVaXLl2sgoKCYJd0023evLnGP/+xY8cGu7Sb6kr/TVixYkWwS7vpHnnkEe/fi9tuu83q1auXtWHDhmCXVWt4DgoAADBO/brADwAA6gUCCgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4xBQAACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAY5/8AL42UmAn8p2oAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_val = torch.histc(qkv, bins = 200, min=-3, max=3)\n",
    "x_val = np.arange(-1, 1, 0.01) *3\n",
    "plt.bar(x_val, y_val, align='center',color=['forestgreen'])\n",
    "plt.title('qkv distribution')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题：为什么要查看 qkv 的分布？**\n",
    "\n",
    "答案：查看 qkv 的分布有助于我们理解模型的内部状态和行为。例如，如果分布非常偏斜或集中在某个特定的范围内，那么可能需要调整模型参数或使用不同的初始化策略。此外，这也可以帮助我们调试模型，确认是否有意外的行为（如值的爆炸或消失）。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问：qkv只是一个[1, 4, 1536]第向量，如何计算分布的？**\n",
    "\n",
    "torch.histc 函数会将张量视为一个一维的数据流，并对所有的数据点进行统计。\n",
    "\n",
    "为了更好地理解这个过程，我们可以将其拆解为以下步骤：\n",
    "\n",
    "首先，torch.histc 函数会将 qkv 张量中的所有元素拉平成一个一维的数组。在例子中，得到一个具有 1 * 4 * 1536 = 6144 个元素的数组。\n",
    "接着，torch.histc 函数会对这个数组中的元素进行分箱统计。bins 参数定义了分箱的数量，min 和 max 参数定义了分箱的范围。在你的例子中，这将把所有的元素分到 200 个箱子中，每个箱子的范围是 (-3, 3)。\n",
    "最后，torch.histc 函数会返回一个数组，其中每个元素表示对应箱子中的元素数量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8\n",
    "head_dim = d_model // num_heads # 1536/8 = 192\n",
    "qkv = qkv.reshape(batch_size, sequence_length, num_heads, 3 * head_dim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qkv 被重塑和拆分成 num_heads 个头，每个头的维度是 head_dim。每个头使用的 q、k、v 是全体 q、k、v 经过线性变换后的部分数据，而不是直接从原始的 q、k、v 中取一部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 192])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题：为什么要改变 qkv 的形状？**\n",
    "\n",
    "答案：在多头注意力机制中，输入的 d_model 维度的数据会被分成多个“头”，每个头处理一部分信息。这样可以让模型在处理输入时更加灵活，因为每个头可以学习并专注于捕获不同的信息"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题：为什么在最后一个维度上将数据划分为了 num_heads 个部分？**\n",
    "\n",
    "答案：在深度学习模型中，最后一个维度通常用来表示数据的特征，而前面的维度则用来表示数据的结构。在这个上下文中，num_heads 是我们的“头”的数量，每个“头”都需要处理一部分特征。因此，我们在最后一个维度上将数据划分为了 num_heads 个部分。\n",
    "\n",
    "这样做的好处是，可以让我们在后续的计算中，更方便地处理每个“头”的数据。例如，当我们需要计算每个“头”的注意力权重时，我们可以直接对最后一个维度（即 3 * head_dim）进行操作，而不需要关心“头”的数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 192])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv = qkv .permute(0,2,1,3)\n",
    "qkv.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过将 qkv 的维度重排列为 `(batch_size, num_heads, sequence_length, 3 * head_dim)`，我们可以更方便地处理每个头的数据，因为现在每个头的数据都在连续的内存位置中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q, k, v = qkv.chunk(3, dim = -1)\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Attention for multiple heads\n",
    "\n",
    "For a single head:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\text { self attention } & =\\operatorname{softmax}\\left(\\frac{Q \\cdot K^T}{\\sqrt{d_k}}+M\\right) \\\\\n",
    "\\text { new } \\mathrm{V} & =\\text { self attention. } V\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "d_k = q.size()[-1]\n",
    "scaled = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "scaled.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题：为什么要计算缩放的点积注意力，以及为什么要对结果进行缩放？**\n",
    "\n",
    "答案：在 Transformer 模型的注意力机制中，我们使用查询 `q` 和键 `k` 的点积来计算注意力权重。然而，当查询和键的维度 `d_k` 较大时，点积的结果可能会非常大，这会导致 softmax 函数（通常用于计算注意力权重）在反向传播时梯度消失，从而影响模型的训练。\n",
    "\n",
    "为了解决这个问题，我们引入了缩放因子 $1/\\sqrt(d_k)$。这个缩放因子可以确保点积的结果在一个合理的范围内，从而避免 softmax 函数的梯度消失问题。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至于 scaled 的形状 `(batch_size, num_heads, sequence_length, sequence_length)`。在最后两个维度上进行的矩阵乘法，即对每个头和每个批次的数据，我们都计算了所有查询和键的点积。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7805, -0.8075],\n",
       "        [-1.6576, -1.1753],\n",
       "        [-0.4163,  0.5748]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.randn(2, 3)\n",
    "torch.transpose(y, 1, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题：为什么多维度张量需要使用 PyTorch 的 transpose 或 permute 函数进行转置，而不能使用 NumPy 的 T 属性？**\n",
    "\n",
    "答案：NumPy 的 T 属性和 transpose 函数在处理二维数组（即矩阵）时表现得很好，它们可以将二维数组的行和列进行交换。然而，对于高于二维的数组，T 属性只会反转维度的顺序，这可能并不是我们想要的结果。\n",
    "\n",
    "例如，对于一个形状为 (a, b, c) 的三维数组，T 属性会得到一个形状为 (c, b, a) 的数组。但在许多情况下，我们可能希望交换其他两个维度，如 (a, c, b) 或 (b, a, c)，这时就需要使用 transpose 或 permute 函数了。\n",
    "\n",
    "PyTorch 的 transpose 和 permute 函数提供了更强大的功能，它们可以交换张量的任意两个维度，或者重新排列所有维度的顺序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf],\n",
       "        [0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.full(scaled.size(), float('-inf'))\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "mask[0][1] # mask for input to a single head"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题：mask[0][1] 具体表示什么？**\n",
    "\n",
    "答案：在这个上下文中，mask[0][1] 用于选择 mask 张量中第一个批次（batch）的第二个头（head）的遮罩。在多头注意力机制中，每个批次包含多个头，每个头都有自己的查询、键和值向量，以及对应的遮罩。这里，mask[0][1] 就是选择了第一个批次的第二个头的遮罩。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1941,    -inf,    -inf,    -inf],\n",
       "        [ 0.3471,  0.3118,    -inf,    -inf],\n",
       "        [-0.2590,  0.0068, -0.0838,    -inf],\n",
       "        [ 0.2202,  0.2823,  0.3527, -0.1306]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(scaled + mask) [0][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题：为什么需要创建这样的 mask 张量，它的作用是什么？**\n",
    "\n",
    "答案：这个 mask 张量是用于遮罩序列的一部分，防止模型看到序列的未来信息。在自然语言处理任务中，特别是在训练语言模型时，我们希望模型在预测下一个词时，只能看到当前词及其之前的词，而不能看到未来的词。这样的遮罩可以帮助我们实现这个目标。\n",
    "\n",
    "具体来说，mask 张量的上三角部分是负无穷，表示序列的未来信息；下三角部分是零，表示序列的当前和过去的信息。当我们在计算注意力权重时，将这个 mask 添加到 scaled 上，因为 softmax 函数对于负无穷的输入会输出零，所以模型就无法看到序列的未来信息了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled = scaled + mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5088, 0.4912, 0.0000, 0.0000],\n",
       "        [0.2860, 0.3732, 0.3408, 0.0000],\n",
       "        [0.2558, 0.2721, 0.2920, 0.1801]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = F.softmax(scaled, dim=-1)\n",
    "attention[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 64])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = torch.matmul(attention, v)\n",
    "values.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k=q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1,-2))/math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled,dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, attention = scaled_dot_product(q, k, v, mask=None)\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2112, 0.3360, 0.3056, 0.1473],\n",
       "        [0.3646, 0.3520, 0.1323, 0.1511],\n",
       "        [0.2257, 0.2945, 0.2690, 0.2108],\n",
       "        [0.2558, 0.2721, 0.2920, 0.1801]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 64])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = values.reshape(batch_size, sequence_length, num_heads*head_dim)\n",
    "values.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=512, bias=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer = nn.Linear(d_model,d_model)\n",
    "linear_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = linear_layer(values)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1040,  0.1237,  0.0365,  ...,  0.0254,  0.2057,  0.1023],\n",
       "         [ 0.0542,  0.2107,  0.2195,  ..., -0.0147, -0.1602,  0.3932],\n",
       "         [-0.0821,  0.1747, -0.3093,  ...,  0.3298,  0.1966,  0.0191],\n",
       "         [-0.0452,  0.0256, -0.0232,  ...,  0.1425,  0.1206,  0.0033]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (All) MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "def scaled_dot_product(q, k ,v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.head_dim = d_model //num_heads\n",
    "        self.qkv_layer = nn.Linear(input_dim, 3*d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, mask = None):\n",
    "        batch_size, sequence_length, input_dim = x.size()\n",
    "        print(f\"x.size(): {x.size()}\")\n",
    "\n",
    "        qkv = self.qkv_layer(x)\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3*self.head_dim)\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "\n",
    "        qkv  = qkv.permute(0, 2, 1, 3)\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "\n",
    "        q, k, v = qkv.chunk(3, dim = -1)\n",
    "        print(f\"q size:{q.size()}, k size:{k.size()}, v size:{v.size()}\")\n",
    "\n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "        print(f\"values.size(): {values.size()}, attention size: {attention.size()}\")\n",
    "\n",
    "        values = values.reshape(batch_size, sequence_length, self.num_heads*self.head_dim)\n",
    "        print(f\"values.size(): {values.size()}\")\n",
    "\n",
    "        out = self.linear_layer(values)\n",
    "        print(f\"out.size(): {out.size()}\")\n",
    "        return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input without mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size(): torch.Size([30, 5, 1024])\n",
      "qkv.size(): torch.Size([30, 5, 1536])\n",
      "qkv.size(): torch.Size([30, 5, 8, 192])\n",
      "qkv.size(): torch.Size([30, 8, 5, 192])\n",
      "q size:torch.Size([30, 8, 5, 64]), k size:torch.Size([30, 8, 5, 64]), v size:torch.Size([30, 8, 5, 64])\n",
      "values.size(): torch.Size([30, 8, 5, 64]), attention size: torch.Size([30, 8, 5, 5])\n",
      "values.size(): torch.Size([30, 5, 512])\n",
      "out.size(): torch.Size([30, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "input_dim = 1024\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "\n",
    "batch_size = 30\n",
    "sequence_length = 5\n",
    "x = torch.randn((batch_size, sequence_length, input_dim))\n",
    "\n",
    "model = MultiheadAttention(input_dim, d_model, num_heads)\n",
    "out = model.forward(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input with mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size(): torch.Size([30, 5, 1024])\n",
      "qkv.size(): torch.Size([30, 5, 1536])\n",
      "qkv.size(): torch.Size([30, 5, 8, 192])\n",
      "qkv.size(): torch.Size([30, 8, 5, 192])\n",
      "q size:torch.Size([30, 8, 5, 64]), k size:torch.Size([30, 8, 5, 64]), v size:torch.Size([30, 8, 5, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn((batch_size, sequence_length, input_dim))\n\u001b[1;32m      9\u001b[0m model \u001b[39m=\u001b[39m MultiheadAttention(input_dim, d_model, num_heads)\n\u001b[0;32m---> 10\u001b[0m out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(x, mask)\n",
      "Cell \u001b[0;32mIn[48], line 42\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     39\u001b[0m q, k, v \u001b[39m=\u001b[39m qkv\u001b[39m.\u001b[39mchunk(\u001b[39m3\u001b[39m, dim \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mq size:\u001b[39m\u001b[39m{\u001b[39;00mq\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m, k size:\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m, v size:\u001b[39m\u001b[39m{\u001b[39;00mv\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m values, attention \u001b[39m=\u001b[39m scaled_dot_product(q, k, v, mask)\n\u001b[1;32m     43\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalues.size(): \u001b[39m\u001b[39m{\u001b[39;00mvalues\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m, attention size: \u001b[39m\u001b[39m{\u001b[39;00mattention\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mreshape(batch_size, sequence_length, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n",
      "Cell \u001b[0;32mIn[48], line 9\u001b[0m, in \u001b[0;36mscaled_dot_product\u001b[0;34m(q, k, v, mask)\u001b[0m\n\u001b[1;32m      7\u001b[0m scaled \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(q, k\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)) \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(d_k)\n\u001b[1;32m      8\u001b[0m \u001b[39mif\u001b[39;00m mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m     scaled \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m mask\n\u001b[1;32m     10\u001b[0m attention \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(scaled, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m values \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(attention, v)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "input_dim = 1024\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "\n",
    "batch_size = 30\n",
    "sequence_length = 5\n",
    "x = torch.randn((batch_size, sequence_length, input_dim))\n",
    "\n",
    "model = MultiheadAttention(input_dim, d_model, num_heads)\n",
    "\n",
    "out = model.forward(x, mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch API\n",
    "\n",
    "- [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
    "- [torch.reshape](https://pytorch.org/docs/stable/generated/torch.reshape.html?highlight=reshape#torch.reshape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

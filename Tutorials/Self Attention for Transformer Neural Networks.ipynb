{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Insights\n",
    "\n",
    "[01 Self Attention in Transformer Neural Networks (with Code!)](https://www.youtube.com/watch?v=QCJQG4DuHT0)\n",
    "\n",
    "\n",
    "üîÑ Bi-directional recurrent neural networks suffer from the limitation of looking at left to right and right to left context separately, potentially missing out on important information.\n",
    "\n",
    "üí° Attention helps determine which parts of the input sentence each word needs to focus on, allowing for more effective incorporation of context and improving the overall feature vector representation.\n",
    "\n",
    "üí° Transformers can be used for various sequence to sequence tasks, such as translating from English to other languages like Kannada.\n",
    "\n",
    "üß† The attention mechanism in the Transformer architecture helps generate higher quality and more context-aware vectors, overcoming the disadvantages of slow training in recurrent neural networks.\n",
    "\n",
    "üß† Applying a mask in the attention function allows the model to focus only on relevant words and ignore context after a certain point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "L, d_k, d_v = 4,8,8  #seq_len,d_k,d_v\n",
    "q = np.random.randn(L, d_k) \n",
    "k = np.random.randn(L, d_k) \n",
    "v = np.random.randn(L, d_v) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 0.93005057 -0.79701316 -0.22866933  1.06596114 -1.42342832 -0.8891726\n",
      "  -0.03123426 -1.18051679]\n",
      " [ 0.08437427 -0.70255813  0.05857633  0.92003131  2.00003795  0.82948422\n",
      "  -0.52968141 -0.85584601]\n",
      " [-0.80326185  0.46506673 -0.45196857 -1.57426684  0.27630791  0.12351596\n",
      "   0.11217196 -1.68406874]\n",
      " [-0.89171967 -0.28107851  0.02281117  1.15788562 -0.29592827 -1.64883181\n",
      "  -0.35067397 -0.41625857]]\n",
      "K\n",
      " [[-3.31827659e-01 -1.44672117e-01 -2.22624694e-01  1.21953293e-01\n",
      "   1.39899895e+00  4.26960040e-01 -4.79485000e-01 -1.65315233e-01]\n",
      " [-1.67427059e-01  1.72024470e-03  8.68835595e-01  1.88825191e+00\n",
      "   7.11572054e-01  6.07053026e-03 -3.98780916e-01 -1.05044594e+00]\n",
      " [-8.37955031e-01  1.54644995e-01  8.73803326e-02  1.63268236e+00\n",
      "   1.24830978e+00 -2.41390942e-01  7.14414816e-01  8.34320116e-01]\n",
      " [-8.37804868e-01 -2.72736424e-01 -2.42600315e-01 -1.40852362e+00\n",
      "   1.99673570e+00 -2.61595430e-01  9.29500353e-01  2.03795632e-02]]\n",
      "V\n",
      " [[-0.49978462 -0.85906258  1.09441924 -0.22010661  1.04392234  0.653697\n",
      "   0.94631293  0.44715966]\n",
      " [ 0.01309478 -1.43901571  0.47461646  1.26289208 -1.9705294  -0.30194412\n",
      "  -0.4281863  -2.04666523]\n",
      " [ 1.07252581  1.05962982  0.44808896  0.17161306 -1.80158358  0.23710998\n",
      "  -0.15248516  0.18719277]\n",
      " [-0.66374852  1.40016388  0.43907347 -4.32385169 -1.16341638 -0.01318457\n",
      "   1.24810734  1.83546834]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q\\n\",q)\n",
    "print(\"K\\n\",k)\n",
    "print(\"V\\n\",v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention\n",
    "\n",
    "$$\n",
    "\\text { self attention }=\\operatorname{softmax}\\left(\\frac{Q \\cdot K^T}{\\sqrt{d_k}}+M\\right) V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.17328813,  1.89129554, -1.75168432, -4.67047989],\n",
       "       [ 3.72046944,  4.3112623 ,  2.53186353,  2.07760462],\n",
       "       [ 0.77180294, -1.30835704, -2.8745662 ,  3.46252174],\n",
       "       [-0.40834074,  2.71152901,  2.0269896 , -1.30669423]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(q, k.T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Á¨¨iË°åÊúÄÂ§ßÂÄºÔºå‰ª£Ë°®ÂíåÁ¨¨i‰∏™wordsÁõ∏ÂÖ≥Â∫¶ÊúÄÈ´ò\n",
    "- $\\sqrt{d_k}$ ‰∏∫‰∫ÜÂáèÂ∞ëÊ≥®ÊÑèÂäõÁü©ÈòµÊï∞ÂÄºÁöÑÊñπÂ∑ÆÔºåËÆ≠ÁªÉËøáÁ®ãÊõ¥Âä†Á®≥ÂÆö\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7571056926026465, 0.6735425966378498, 6.697288483667932)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var(), k.var(), np.matmul(q, k.T).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7571056926026465, 0.6735425966378498, 0.8371610604584914)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "q.var(), k.var(), scaled.var() # ‰ΩéÊñπÂ∑Æ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking\n",
    "\n",
    "- This is to ensure words don't get context from words generated in the future.\n",
    "- No required in the encoders, but required in the decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.tril(np.ones((L,L)))\n",
    "mask  # Âè™ËÉΩÁúãÂà∞ÂâçÈù¢ÁöÑÂ≠óÁ¨¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[mask == 0] = -np.infty\n",
    "mask[mask == 1] = 0\n",
    "\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.76837339,        -inf,        -inf,        -inf],\n",
       "       [ 1.31538459,  1.5242614 ,        -inf,        -inf],\n",
       "       [ 0.27287355, -0.46257407, -1.01631263,        -inf],\n",
       "       [-0.14437025,  0.95867027,  0.71664904, -0.46198618]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled + mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax\n",
    "\n",
    "$$\n",
    "\\operatorname{softmax}=\\frac{e^{x_i}}{\\sum_j e_j^x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.44796983, 0.55203017, 0.        , 0.        ],\n",
       "       [0.56987013, 0.27313355, 0.15699631, 0.        ],\n",
       "       [0.14071096, 0.42400632, 0.33286171, 0.10242101]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = softmax(scaled + mask)\n",
    "attention  # Ë°å‰πãÂíå‰∏∫1Ôºå ‰ª£Ë°®Ê¶ÇÁéáÂàÜÂ∏É"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14743653, 0.62045006, 0.17113577, 0.06097764],\n",
       "       [0.28997163, 0.35733007, 0.19048005, 0.16221825],\n",
       "       [0.2302083 , 0.11033674, 0.06342121, 0.59603375],\n",
       "       [0.14071096, 0.42400632, 0.33286171, 0.10242101]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = softmax(scaled) # no mask \n",
    "attention  # Ë°å‰πãÂíå‰∏∫1Ôºå ‰ª£Ë°®Ê¶ÇÁéáÂàÜÂ∏É"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07751186, -0.75277534,  0.5592909 ,  0.51682059, -1.44796057,\n",
       "        -0.0511884 , -0.07613615, -1.059968  ],\n",
       "       [-0.04362156, -0.33433688,  0.643523  , -0.28127416, -0.93331464,\n",
       "         0.12468582,  0.29482048, -0.26826846],\n",
       "       [-0.44120537,  0.54520831,  0.59443298, -2.47760462, -0.78479624,\n",
       "         0.1243503 ,  0.90484772,  0.98299058],\n",
       "       [ 0.22424808, -0.23491487,  0.54935916,  0.11877295, -1.40746208,\n",
       "         0.04153057,  0.02867885, -0.55457889]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v = np.matmul(attention, v)\n",
    "new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.49978462, -0.85906258,  1.09441924, -0.22010661,  1.04392234,\n",
       "         0.653697  ,  0.94631293,  0.44715966],\n",
       "       [ 0.01309478, -1.43901571,  0.47461646,  1.26289208, -1.9705294 ,\n",
       "        -0.30194412, -0.4281863 , -2.04666523],\n",
       "       [ 1.07252581,  1.05962982,  0.44808896,  0.17161306, -1.80158358,\n",
       "         0.23710998, -0.15248516,  0.18719277],\n",
       "       [-0.66374852,  1.40016388,  0.43907347, -4.32385169, -1.16341638,\n",
       "        -0.01318457,  1.24810734,  1.83546834]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v # before attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# self-attention calculation (masked selection)\n",
    "**`k` and `v` scaled_dot_product_attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask = None):\n",
    "    d_k = q.shape[-1]\n",
    "    scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled = scaled + mask\n",
    "    \n",
    "    attention = softmax(scaled)\n",
    "    out = np.matmul(attention, v)\n",
    "    return out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 0.93005057 -0.79701316 -0.22866933  1.06596114 -1.42342832 -0.8891726\n",
      "  -0.03123426 -1.18051679]\n",
      " [ 0.08437427 -0.70255813  0.05857633  0.92003131  2.00003795  0.82948422\n",
      "  -0.52968141 -0.85584601]\n",
      " [-0.80326185  0.46506673 -0.45196857 -1.57426684  0.27630791  0.12351596\n",
      "   0.11217196 -1.68406874]\n",
      " [-0.89171967 -0.28107851  0.02281117  1.15788562 -0.29592827 -1.64883181\n",
      "  -0.35067397 -0.41625857]]\n",
      "K\n",
      " [[-3.31827659e-01 -1.44672117e-01 -2.22624694e-01  1.21953293e-01\n",
      "   1.39899895e+00  4.26960040e-01 -4.79485000e-01 -1.65315233e-01]\n",
      " [-1.67427059e-01  1.72024470e-03  8.68835595e-01  1.88825191e+00\n",
      "   7.11572054e-01  6.07053026e-03 -3.98780916e-01 -1.05044594e+00]\n",
      " [-8.37955031e-01  1.54644995e-01  8.73803326e-02  1.63268236e+00\n",
      "   1.24830978e+00 -2.41390942e-01  7.14414816e-01  8.34320116e-01]\n",
      " [-8.37804868e-01 -2.72736424e-01 -2.42600315e-01 -1.40852362e+00\n",
      "   1.99673570e+00 -2.61595430e-01  9.29500353e-01  2.03795632e-02]]\n",
      "V\n",
      " [[-0.49978462 -0.85906258  1.09441924 -0.22010661  1.04392234  0.653697\n",
      "   0.94631293  0.44715966]\n",
      " [ 0.01309478 -1.43901571  0.47461646  1.26289208 -1.9705294  -0.30194412\n",
      "  -0.4281863  -2.04666523]\n",
      " [ 1.07252581  1.05962982  0.44808896  0.17161306 -1.80158358  0.23710998\n",
      "  -0.15248516  0.18719277]\n",
      " [-0.66374852  1.40016388  0.43907347 -4.32385169 -1.16341638 -0.01318457\n",
      "   1.24810734  1.83546834]]\n",
      "New V\n",
      " [[ 0.07751186 -0.75277534  0.5592909   0.51682059 -1.44796057 -0.0511884\n",
      "  -0.07613615 -1.059968  ]\n",
      " [-0.04362156 -0.33433688  0.643523   -0.28127416 -0.93331464  0.12468582\n",
      "   0.29482048 -0.26826846]\n",
      " [-0.44120537  0.54520831  0.59443298 -2.47760462 -0.78479624  0.1243503\n",
      "   0.90484772  0.98299058]\n",
      " [ 0.22424808 -0.23491487  0.54935916  0.11877295 -1.40746208  0.04153057\n",
      "   0.02867885 -0.55457889]]\n",
      "Attention\n",
      " [[0.14743653 0.62045006 0.17113577 0.06097764]\n",
      " [0.28997163 0.35733007 0.19048005 0.16221825]\n",
      " [0.2302083  0.11033674 0.06342121 0.59603375]\n",
      " [0.14071096 0.42400632 0.33286171 0.10242101]]\n"
     ]
    }
   ],
   "source": [
    "# no masked self- attention\n",
    "values, attention = scaled_dot_product_attention(q, k, v, mask = None)\n",
    "print(\"Q\\n\",q)\n",
    "print(\"K\\n\",k)\n",
    "print(\"V\\n\",v)\n",
    "print(\"New V\\n\",values)\n",
    "print(\"Attention\\n\",attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 0.93005057 -0.79701316 -0.22866933  1.06596114 -1.42342832 -0.8891726\n",
      "  -0.03123426 -1.18051679]\n",
      " [ 0.08437427 -0.70255813  0.05857633  0.92003131  2.00003795  0.82948422\n",
      "  -0.52968141 -0.85584601]\n",
      " [-0.80326185  0.46506673 -0.45196857 -1.57426684  0.27630791  0.12351596\n",
      "   0.11217196 -1.68406874]\n",
      " [-0.89171967 -0.28107851  0.02281117  1.15788562 -0.29592827 -1.64883181\n",
      "  -0.35067397 -0.41625857]]\n",
      "K\n",
      " [[-3.31827659e-01 -1.44672117e-01 -2.22624694e-01  1.21953293e-01\n",
      "   1.39899895e+00  4.26960040e-01 -4.79485000e-01 -1.65315233e-01]\n",
      " [-1.67427059e-01  1.72024470e-03  8.68835595e-01  1.88825191e+00\n",
      "   7.11572054e-01  6.07053026e-03 -3.98780916e-01 -1.05044594e+00]\n",
      " [-8.37955031e-01  1.54644995e-01  8.73803326e-02  1.63268236e+00\n",
      "   1.24830978e+00 -2.41390942e-01  7.14414816e-01  8.34320116e-01]\n",
      " [-8.37804868e-01 -2.72736424e-01 -2.42600315e-01 -1.40852362e+00\n",
      "   1.99673570e+00 -2.61595430e-01  9.29500353e-01  2.03795632e-02]]\n",
      "V\n",
      " [[-0.49978462 -0.85906258  1.09441924 -0.22010661  1.04392234  0.653697\n",
      "   0.94631293  0.44715966]\n",
      " [ 0.01309478 -1.43901571  0.47461646  1.26289208 -1.9705294  -0.30194412\n",
      "  -0.4281863  -2.04666523]\n",
      " [ 1.07252581  1.05962982  0.44808896  0.17161306 -1.80158358  0.23710998\n",
      "  -0.15248516  0.18719277]\n",
      " [-0.66374852  1.40016388  0.43907347 -4.32385169 -1.16341638 -0.01318457\n",
      "   1.24810734  1.83546834]]\n",
      "New V\n",
      " [[-0.49978462 -0.85906258  1.09441924 -0.22010661  1.04392234  0.653697\n",
      "   0.94631293  0.44715966]\n",
      " [-0.21665972 -1.1792142   0.75226941  0.59855341 -0.62014597  0.12615427\n",
      "   0.18754788 -0.92950692]\n",
      " [-0.11285311 -0.7162396   0.82365883  0.24644863 -0.22615951  0.32727672\n",
      "   0.39838382 -0.27480144]\n",
      " [ 0.22424808 -0.23491487  0.54935916  0.11877295 -1.40746208  0.04153057\n",
      "   0.02867885 -0.55457889]]\n",
      "Attention\n",
      " [[1.         0.         0.         0.        ]\n",
      " [0.44796983 0.55203017 0.         0.        ]\n",
      " [0.56987013 0.27313355 0.15699631 0.        ]\n",
      " [0.14071096 0.42400632 0.33286171 0.10242101]]\n"
     ]
    }
   ],
   "source": [
    "# masked self- attention\n",
    "# no masked self- attention\n",
    "values, attention = scaled_dot_product_attention(q, k, v, mask = mask)\n",
    "print(\"Q\\n\",q)\n",
    "print(\"K\\n\",k)\n",
    "print(\"V\\n\",v)\n",
    "print(\"New V\\n\",values)\n",
    "print(\"Attention\\n\",attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

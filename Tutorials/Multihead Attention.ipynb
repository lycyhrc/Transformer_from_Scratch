{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keypoints\n",
    "\n",
    "💻 The multi-head attention mechanism is a crucial component of the Transformer architecture, and understanding its theory and implementation is essential for coding it effectively.\n",
    "\n",
    "💡 The multi-head attention system generates multiple attention matrices, each representing a probability distribution, to capture different aspects of the input sequence.\n",
    "\n",
    "🧠 The head dimension in multi-head attention exists because it combines the query, key, and value vectors, allowing for parallel operations on the sequences.\n",
    "\n",
    "🧯 Combining multiple heads in a transformer neural network allows them to communicate with each other, resulting in a more context-aware output vector.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length = 4\n",
    "batch_size = 1\n",
    "input_dim = 512\n",
    "d_model = 512\n",
    "x = torch.randn((batch_size, sequence_length, input_dim)) # x : bt_size, seq_len, input_dim\n",
    "x.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入数据形状为 `(batch_size, sequence_length, input_dim)`。这表示在每个epochs中，有 batch_size 个样本，每个样本是一个长度为 sequence_length 的序列，每个元素的维度为 input_dim。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv_layer = nn.Linear(input_dim, 3 * d_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题：为什么要定义一个全连接层，而且输出维度为什么是 3 * d_model？**\n",
    "\n",
    "答案：因为需要从同一个输入 x 生成三个不同的向量：查询（Query）、键（Key）和值（Value）。这三个向量都通过 qkv_layer 生成，所以 qkv_layer 的输出实际上是这三个向量的集合。(一次生成，后续分开)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1536])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv = qkv_layer(x)  # linear只有最会维度进行变化 （*，H_in）---> (*,H_out)\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'qkv distribution')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_val = torch.histc(qkv, bins = 200, min=-3, max=3)\n",
    "x_val = np.arange(-1, 1, 0.01) *3\n",
    "plt.bar(x_val, y_val, align='center',color=['forestgreen'])\n",
    "plt.title('qkv distribution')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题：为什么要查看 qkv 的分布？**\n",
    "\n",
    "答案：查看 qkv 的分布有助于我们理解模型的内部状态和行为。例如，如果分布非常偏斜或集中在某个特定的范围内，那么可能需要调整模型参数或使用不同的初始化策略。此外，这也可以帮助我们调试模型，确认是否有意外的行为（如值的爆炸或消失）。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问：qkv只是一个[1, 4, 1536]第向量，如何计算分布的？**\n",
    "\n",
    "torch.histc 函数会将张量视为一个一维的数据流，并对所有的数据点进行统计。\n",
    "\n",
    "为了更好地理解这个过程，我们可以将其拆解为以下步骤：\n",
    "\n",
    "首先，torch.histc 函数会将 qkv 张量中的所有元素拉平成一个一维的数组。在例子中，得到一个具有 1 * 4 * 1536 = 6144 个元素的数组。\n",
    "接着，torch.histc 函数会对这个数组中的元素进行分箱统计。bins 参数定义了分箱的数量，min 和 max 参数定义了分箱的范围。在你的例子中，这将把所有的元素分到 200 个箱子中，每个箱子的范围是 (-3, 3)。\n",
    "最后，torch.histc 函数会返回一个数组，其中每个元素表示对应箱子中的元素数量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 192])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_heads = 8\n",
    "head_dim = d_model // num_heads # 1536/8 = 192\n",
    "qkv = qkv.reshape(batch_size, sequence_length, num_heads, 3 * head_dim)\n",
    "qkv.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qkv 被重塑和拆分成 num_heads 个头，每个头的维度是 head_dim。每个头使用的 q、k、v 是全体 q、k、v 经过线性变换后的部分数据，而不是直接从原始的 q、k、v 中取一部分"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题：为什么要改变 qkv 的形状？**\n",
    "\n",
    "答案：在多头注意力机制中，输入的 d_model 维度的数据会被分成多个“头”，每个头处理一部分信息。这样可以让模型在处理输入时更加灵活，因为每个头可以学习并专注于捕获不同的信息"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题：为什么在最后一个维度上将数据划分为了 num_heads 个部分？**\n",
    "\n",
    "答案：在深度学习模型中，最后一个维度通常用来表示数据的特征，而前面的维度则用来表示数据的结构。在这个上下文中，num_heads 是我们的“头”的数量，每个“头”都需要处理一部分特征。因此，我们在最后一个维度上将数据划分为了 num_heads 个部分。\n",
    "\n",
    "这样做的好处是，可以让我们在后续的计算中，更方便地处理每个“头”的数据。例如，当我们需要计算每个“头”的注意力权重时，我们可以直接对最后一个维度（即 3 * head_dim）进行操作，而不需要关心“头”的数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 192])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv = qkv.permute(0,2,1,3)\n",
    "qkv.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过将 qkv 的维度重排列为 `(batch_size, num_heads, sequence_length, 3 * head_dim)`，我们可以更方便地处理每个头的数据，因为现在每个头的数据都在连续的内存位置中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q, k, v = qkv.chunk(3, dim = -1)\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Attention for multiple heads\n",
    "\n",
    "For a single head:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\text { self attention } & =\\operatorname{softmax}\\left(\\frac{Q \\cdot K^T}{\\sqrt{d_k}}+M\\right) \\\\\n",
    "\\text { new } \\mathrm{V} & =\\text { self attention. } V\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "d_k = q.size()[-1]\n",
    "scaled = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # 1，8，4，64 * 1，8，64，4 ---> 1,8,4,4\n",
    "scaled.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题：为什么要计算缩放的点积注意力，以及为什么要对结果进行缩放？**\n",
    "\n",
    "答案：在 Transformer 模型的注意力机制中，我们使用查询 `q` 和键 `k` 的点积来计算注意力权重。然而，当查询和键的维度 $d_k$ 较大时，点积的结果可能会非常大，这会导致 softmax 函数（通常用于计算注意力权重）在反向传播时梯度消失，从而影响模型的训练。\n",
    "\n",
    "为了解决这个问题，我们引入了缩放因子 $1/\\sqrt(d_k)$。这个缩放因子可以确保点积的结果在一个合理的范围内，从而避免 softmax 函数的梯度消失问题。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaled 的形状 `(batch_size, num_heads, sequence_length, sequence_length)`。因为这是在最后两个维度上进行的矩阵乘法，即对于每个头和每个批次的数据，我们都计算了所有查询和键的点积。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: tensor([[ 0.9286, -0.2131, -1.0884],\n",
      "        [ 1.2531,  0.7147, -0.6907]])\n",
      "y transpose: tensor([[ 0.9286,  1.2531],\n",
      "        [-0.2131,  0.7147],\n",
      "        [-1.0884, -0.6907]])\n"
     ]
    }
   ],
   "source": [
    "# 补充\n",
    "y = torch.randn(2, 3)\n",
    "print(f\"y: {y}\")\n",
    "print(f\"y transpose: {torch.transpose(y, 1, 0)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题：为什么多维度张量需要使用 PyTorch 的 transpose 或 permute 函数进行转置，而不能使用 NumPy 的 T 属性？**\n",
    "\n",
    "答案：NumPy 的 T 属性和 transpose 函数在处理二维数组（即矩阵）时表现得很好，它们可以将二维数组的行和列进行交换。然而，对于高于二维的数组，T 属性只会反转维度的顺序，这可能并不是我们想要的结果。\n",
    "\n",
    "例如，对于一个形状为 (a, b, c) 的三维数组，T 属性会得到一个形状为 (c, b, a) 的数组。但在许多情况下，我们可能希望交换其他两个维度，如 (a, c, b) 或 (b, a, c)，这时就需要使用 transpose 或 permute 函数了。\n",
    "\n",
    "PyTorch 的 transpose 和 permute 函数提供了更强大的功能，它们可以交换张量的任意两个维度，或者重新排列所有维度的顺序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All head masked: tensor([[[[0., -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., -inf, -inf, -inf],\n",
      "          [0., 0., -inf, -inf],\n",
      "          [0., 0., 0., -inf],\n",
      "          [0., 0., 0., 0.]]]])\n",
      "Single head masked: tensor([[0., -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf],\n",
      "        [0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "## 生成masked\n",
    "mask = torch.full(scaled.size(), float('-inf'))\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "print(f\"All head masked: {mask}\")\n",
    "print(f\"Single head masked: {mask[0][1]}\") # mask for input to a single head"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题：mask[0][1] 具体表示什么？**\n",
    "\n",
    "答案：在这个上下文中，mask[0][1] 用于选择 mask 张量中第一个批次（batch）的第二个头（head）的遮罩。在多头注意力机制中，每个批次包含多个头，每个头都有自己的查询、键和值向量，以及对应的遮罩。这里，mask[0][1] 就是选择了第一个批次的第二个头的遮罩。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2548,    -inf,    -inf,    -inf],\n",
       "        [ 0.7148,  0.7982,    -inf,    -inf],\n",
       "        [ 0.0151,  0.1546,  0.6325,    -inf],\n",
       "        [-0.2733, -0.6332, -0.1531,  0.1882]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(scaled + mask) [0][0] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题：为什么需要创建这样的 mask 张量，它的作用是什么？**\n",
    "\n",
    "答案：这个 mask 张量是用于遮罩序列的一部分，防止模型看到序列的未来信息。在自然语言处理任务中，特别是在训练语言模型时，我们希望模型在预测下一个词时，只能看到当前词及其之前的词，而不能看到未来的词。这样的遮罩可以帮助我们实现这个目标。\n",
    "\n",
    "具体来说，mask 张量的上**三角部分是负无穷，表示序列的未来信息；下三角部分是零**，表示序列的当前和过去的信息。当我们在计算注意力权重时，将这个 mask 添加到 scaled 上，因为 softmax 函数对于负无穷的输入会输出零，所以模型就无法看到序列的未来信息了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled = scaled + mask # multi head  attention score matrix with masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4792, 0.5208, 0.0000, 0.0000],\n",
       "        [0.2498, 0.2871, 0.4631, 0.0000],\n",
       "        [0.2266, 0.1582, 0.2556, 0.3596]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = F.softmax(scaled, dim=-1)  # scaled to [0, 1]\n",
    "attention[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention size: torch.Size([1, 8, 4, 4]), values size: torch.Size([1, 8, 4, 64])\n"
     ]
    }
   ],
   "source": [
    "print(f\"attention size: {attention.size()}, values size: {v.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 64])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = torch.matmul(attention, v) # attention score * values\n",
    "values.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k=q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1,-2))/math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled,dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, attention = scaled_dot_product(q, k, v, mask=None)\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2764, 0.2697, 0.2149, 0.2389],\n",
       "        [0.2994, 0.3255, 0.1295, 0.2455],\n",
       "        [0.2013, 0.2314, 0.3732, 0.1941],\n",
       "        [0.2266, 0.1582, 0.2556, 0.3596]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 64])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = values.reshape(batch_size, sequence_length, num_heads*head_dim)\n",
    "values.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=512, bias=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer = nn.Linear(d_model,d_model)\n",
    "linear_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = linear_layer(values)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3412,  0.2449,  0.2114,  ..., -0.2294,  0.0139,  0.0105],\n",
       "         [-0.1401,  0.0268, -0.0720,  ..., -0.1523,  0.2122,  0.0164],\n",
       "         [-0.0465,  0.1051, -0.0417,  ...,  0.1026,  0.1103, -0.0321],\n",
       "         [-0.0727, -0.0463,  0.0377,  ..., -0.1145,  0.2986,  0.2640]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (All) MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "def scaled_dot_product(q, k ,v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.head_dim = d_model //num_heads\n",
    "        self.qkv_layer = nn.Linear(input_dim, 3*d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, mask = None):\n",
    "        batch_size, sequence_length, input_dim = x.size()\n",
    "        print(f\"x.size(): {x.size()}\")\n",
    "\n",
    "        qkv = self.qkv_layer(x)\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3*self.head_dim)\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "\n",
    "        qkv  = qkv.permute(0, 2, 1, 3)\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "\n",
    "        q, k, v = qkv.chunk(3, dim = -1)\n",
    "        print(f\"q size:{q.size()}, k size:{k.size()}, v size:{v.size()}\")\n",
    "\n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "        print(f\"values.size(): {values.size()}, attention size: {attention.size()}\")\n",
    "\n",
    "        values = values.reshape(batch_size, sequence_length, self.num_heads*self.head_dim)\n",
    "        print(f\"values.size(): {values.size()}\")\n",
    "\n",
    "        out = self.linear_layer(values)\n",
    "        print(f\"out.size(): {out.size()}\")\n",
    "        return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input without mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size(): torch.Size([30, 5, 1024])\n",
      "qkv.size(): torch.Size([30, 5, 1536])\n",
      "qkv.size(): torch.Size([30, 5, 8, 192])\n",
      "qkv.size(): torch.Size([30, 8, 5, 192])\n",
      "q size:torch.Size([30, 8, 5, 64]), k size:torch.Size([30, 8, 5, 64]), v size:torch.Size([30, 8, 5, 64])\n",
      "values.size(): torch.Size([30, 8, 5, 64]), attention size: torch.Size([30, 8, 5, 5])\n",
      "values.size(): torch.Size([30, 5, 512])\n",
      "out.size(): torch.Size([30, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "input_dim = 1024\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "\n",
    "batch_size = 30\n",
    "sequence_length = 5\n",
    "x = torch.randn((batch_size, sequence_length, input_dim))\n",
    "\n",
    "model = MultiheadAttention(input_dim, d_model, num_heads)\n",
    "out = model.forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.5284e-02,  1.2722e-01,  2.4985e-01,  ..., -2.1212e-01,\n",
       "          -1.4147e-01,  5.2860e-02],\n",
       "         [-1.2464e-01, -1.4993e-01, -1.1479e-01,  ..., -2.9906e-02,\n",
       "          -6.3602e-03,  1.9076e-01],\n",
       "         [-1.1769e-01, -9.4000e-02,  9.0091e-02,  ...,  8.0865e-02,\n",
       "           4.6137e-02, -1.1308e-01],\n",
       "         [ 8.9524e-02,  5.5354e-02, -3.9023e-02,  ...,  3.1260e-02,\n",
       "          -1.2758e-01, -3.8482e-02],\n",
       "         [-1.5184e-01, -5.3139e-02, -1.8266e-01,  ..., -1.0104e-01,\n",
       "           3.0766e-01,  1.0210e-01]],\n",
       "\n",
       "        [[-5.6526e-02,  3.8834e-01, -1.2390e-02,  ..., -5.9704e-02,\n",
       "           2.3768e-01, -2.9467e-02],\n",
       "         [ 1.2905e-01,  2.2574e-02,  7.2760e-02,  ...,  3.4676e-03,\n",
       "          -6.2535e-02,  1.4201e-01],\n",
       "         [ 1.7856e-01, -4.9391e-02,  6.3659e-02,  ..., -1.6452e-01,\n",
       "          -9.0021e-02, -1.3106e-01],\n",
       "         [ 1.4727e-01,  9.4759e-03, -2.9079e-01,  ...,  5.3035e-02,\n",
       "          -1.2689e-01,  2.4708e-01],\n",
       "         [-2.3948e-02,  3.7071e-02, -8.5224e-02,  ..., -6.0736e-02,\n",
       "          -4.0356e-02, -2.0263e-01]],\n",
       "\n",
       "        [[-9.9209e-02,  1.4457e-01,  1.2916e-01,  ..., -1.3547e-01,\n",
       "           2.9630e-01, -1.3439e-01],\n",
       "         [ 2.0039e-01, -1.7372e-01,  7.3501e-02,  ..., -2.1457e-01,\n",
       "          -7.8606e-02, -2.2041e-01],\n",
       "         [-1.0347e-02,  6.2543e-03,  1.2217e-01,  ..., -3.8167e-01,\n",
       "          -9.1258e-02, -1.2748e-01],\n",
       "         [ 7.3079e-02, -1.4386e-01, -1.3951e-02,  ..., -1.7582e-01,\n",
       "           5.1970e-02,  5.1082e-02],\n",
       "         [-7.4195e-02, -3.7951e-03, -6.0524e-02,  ..., -1.0862e-01,\n",
       "          -2.0819e-03,  1.7469e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.2739e-01, -4.0455e-02,  2.0003e-01,  ...,  2.0781e-01,\n",
       "           8.3078e-02,  1.4658e-01],\n",
       "         [-1.4010e-01, -1.8082e-01,  6.8605e-02,  ...,  4.8183e-03,\n",
       "          -7.9992e-02,  9.1744e-02],\n",
       "         [-1.4348e-01, -2.8711e-02, -6.9014e-02,  ..., -8.9810e-02,\n",
       "           1.7543e-01,  3.8294e-04],\n",
       "         [-2.8533e-02,  4.4199e-03,  8.1753e-02,  ..., -2.6650e-01,\n",
       "          -6.1190e-02,  1.6527e-01],\n",
       "         [ 9.3710e-02,  1.1338e-02, -1.1004e-02,  ...,  4.5200e-02,\n",
       "           2.1608e-01, -1.0056e-01]],\n",
       "\n",
       "        [[-9.1423e-02,  2.4725e-01, -2.5480e-01,  ...,  7.3778e-02,\n",
       "          -3.7986e-01, -4.4736e-02],\n",
       "         [-2.8415e-01, -2.8046e-02,  1.3098e-01,  ..., -2.2916e-01,\n",
       "          -8.6993e-02, -6.3826e-02],\n",
       "         [ 2.4611e-01, -1.8869e-01,  3.5635e-01,  ...,  1.1057e-02,\n",
       "          -3.1153e-01, -8.1883e-02],\n",
       "         [-7.7056e-02, -7.9658e-02,  1.6744e-01,  ..., -9.7499e-02,\n",
       "           1.0359e-01, -5.0383e-02],\n",
       "         [-1.2270e-01,  1.4218e-01,  8.5103e-02,  ...,  2.4662e-01,\n",
       "          -8.8586e-04, -4.5084e-02]],\n",
       "\n",
       "        [[-2.6870e-01,  1.6422e-01,  4.7651e-02,  ..., -4.9240e-02,\n",
       "           3.5884e-01, -5.4511e-02],\n",
       "         [ 1.7895e-01,  1.7038e-01,  4.1670e-02,  ...,  1.3283e-01,\n",
       "          -2.7424e-01,  1.1805e-01],\n",
       "         [ 2.4595e-01, -3.6187e-02,  1.3643e-01,  ...,  1.1451e-01,\n",
       "           1.0459e-01, -8.9542e-02],\n",
       "         [ 9.0449e-02,  3.3295e-02, -2.9386e-01,  ..., -8.6094e-02,\n",
       "          -1.0355e-01, -2.8783e-02],\n",
       "         [-1.1242e-01,  1.1759e-01, -1.1835e-01,  ..., -9.4781e-02,\n",
       "          -3.0517e-02,  1.1291e-01]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input with mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size(): torch.Size([30, 5, 1024])\n",
      "qkv.size(): torch.Size([30, 5, 1536])\n",
      "qkv.size(): torch.Size([30, 5, 8, 192])\n",
      "qkv.size(): torch.Size([30, 8, 5, 192])\n",
      "q size:torch.Size([30, 8, 5, 64]), k size:torch.Size([30, 8, 5, 64]), v size:torch.Size([30, 8, 5, 64])\n",
      "values.size(): torch.Size([30, 8, 5, 64]), attention size: torch.Size([30, 8, 5, 5])\n",
      "values.size(): torch.Size([30, 5, 512])\n",
      "out.size(): torch.Size([30, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "input_dim = 1024\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "\n",
    "batch_size = 30\n",
    "sequence_length = 5\n",
    "# x = torch.randn((batch_size, sequence_length, input_dim))  add masked\n",
    "\n",
    "model = MultiheadAttention(input_dim, d_model, num_heads)\n",
    "\n",
    "mask = (torch.triu(torch.ones((batch_size, 1, sequence_length, sequence_length))) == 1).transpose(-1, -2)\n",
    "mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "\n",
    "out = model.forward(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3427e-01,  1.8508e-03,  3.1447e-01,  ..., -3.6402e-01,\n",
       "           1.1125e-01, -2.2567e-02],\n",
       "         [-1.1818e-01,  8.7459e-02,  1.8139e-01,  ...,  1.9198e-01,\n",
       "           9.0766e-02,  3.3462e-01],\n",
       "         [ 3.8551e-01, -2.0970e-02, -1.3621e-02,  ...,  6.1666e-02,\n",
       "          -1.3019e-02,  1.5994e-01],\n",
       "         [ 1.2894e-01,  2.1139e-01, -6.2112e-02,  ...,  5.1420e-02,\n",
       "           8.0267e-02,  1.4966e-01],\n",
       "         [-6.8595e-01,  3.3024e-01, -1.0399e-01,  ..., -3.8271e-01,\n",
       "           8.9425e-02, -1.1681e-01]],\n",
       "\n",
       "        [[ 8.5313e-02,  2.0682e-01,  9.9087e-03,  ..., -1.1775e-01,\n",
       "          -2.0251e-01, -3.3115e-01],\n",
       "         [ 4.1339e-01,  8.4770e-02,  1.2837e-03,  ...,  4.0821e-01,\n",
       "          -1.6438e-01,  1.2384e-01],\n",
       "         [-5.5205e-02, -5.0865e-01,  2.9354e-01,  ...,  4.7963e-01,\n",
       "          -1.7520e-01, -2.5475e-02],\n",
       "         [ 2.2127e-01,  1.0342e-01, -2.5137e-01,  ...,  3.1584e-01,\n",
       "          -5.5105e-02, -3.1359e-01],\n",
       "         [-7.9030e-02, -1.8711e-01,  6.1984e-02,  ..., -1.1434e-01,\n",
       "          -1.2354e-01, -1.3375e-01]],\n",
       "\n",
       "        [[ 5.2905e-02, -2.6395e-01, -2.0207e-01,  ..., -2.9690e-01,\n",
       "          -5.6637e-02, -5.0725e-02],\n",
       "         [ 3.7682e-01, -1.8872e-01,  3.8592e-01,  ...,  4.4001e-02,\n",
       "          -3.8609e-04,  3.6950e-01],\n",
       "         [-1.1478e-01,  1.0126e-01,  1.5504e-01,  ...,  2.4204e-02,\n",
       "          -1.4982e-01,  6.6195e-02],\n",
       "         [ 1.9947e-01, -2.2834e-01,  3.1641e-01,  ...,  4.5814e-01,\n",
       "          -5.2437e-02, -8.1786e-02],\n",
       "         [ 8.8090e-02,  2.3792e-01, -6.4968e-02,  ..., -1.8648e-01,\n",
       "          -3.3943e-02, -2.2735e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 9.6233e-03,  1.5329e-01,  2.3566e-01,  ..., -5.0566e-02,\n",
       "           4.1958e-01,  2.4529e-01],\n",
       "         [-2.4768e-01,  5.5671e-02, -3.7979e-02,  ..., -4.3241e-01,\n",
       "          -2.4911e-01, -2.7028e-01],\n",
       "         [ 2.1640e-01,  9.0386e-02,  2.2395e-01,  ...,  2.6138e-01,\n",
       "          -1.0001e-01, -2.7728e-01],\n",
       "         [ 1.8337e-02,  2.5636e-01, -5.3120e-02,  ...,  4.1417e-01,\n",
       "           7.3701e-02, -3.9133e-02],\n",
       "         [-2.2251e-01, -1.7144e-01,  2.3460e-01,  ...,  1.6446e-01,\n",
       "          -4.4295e-02,  1.9147e-01]],\n",
       "\n",
       "        [[ 2.8300e-01,  1.0699e-01, -6.9333e-02,  ..., -6.6002e-01,\n",
       "           8.1981e-02, -2.1227e-01],\n",
       "         [ 2.3102e-01,  2.3151e-01, -9.9720e-02,  ...,  5.7686e-02,\n",
       "           3.1050e-01, -1.3086e-01],\n",
       "         [ 1.7396e-01, -3.9617e-01,  1.8523e-01,  ...,  2.1972e-01,\n",
       "          -6.3581e-02, -9.5748e-02],\n",
       "         [ 1.4541e-01, -1.3795e-01, -5.5526e-02,  ...,  9.4149e-02,\n",
       "          -6.5945e-02, -5.9847e-02],\n",
       "         [ 2.4684e-01, -2.7655e-01,  9.4354e-02,  ..., -2.5030e-01,\n",
       "           2.8620e-01, -8.0246e-02]],\n",
       "\n",
       "        [[ 3.2054e-02, -9.8048e-02, -1.0432e-01,  ...,  1.6767e-01,\n",
       "          -2.9065e-01,  7.2968e-02],\n",
       "         [ 8.0188e-03,  2.1525e-01,  2.2751e-01,  ..., -2.0668e-02,\n",
       "          -3.8956e-04,  4.8769e-02],\n",
       "         [ 4.1621e-01,  2.3312e-01,  1.6848e-01,  ...,  9.4038e-02,\n",
       "          -2.2613e-01, -8.4307e-03],\n",
       "         [-2.4866e-01, -2.3536e-02, -1.3127e-01,  ..., -3.6530e-03,\n",
       "           5.2772e-02, -4.2564e-01],\n",
       "         [-4.3766e-01, -5.6205e-03,  4.7792e-01,  ..., -2.3176e-01,\n",
       "          -3.2930e-01,  4.2408e-01]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch API\n",
    "## torch.nn\n",
    "- [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)：应用线性变换到输入数据：y = xA^T + b\n",
    "\n",
    "## torch\n",
    "- [torch.reshape](https://pytorch.org/docs/stable/generated/torch.reshape.html?highlight=reshape#torch.reshape)：将输入张量的形状改变为给定的形状\n",
    "- [torch.permute](https://pytorch.org/docs/stable/generated/torch.permute.html?highlight=permute#torch.permute)：返回一个输入张量的视图，该视图的维度以特定的顺序重新排列\n",
    "- [torch.chunk](https://pytorch.org/docs/stable/generated/torch.chunk.html?highlight=chunk#torch.chunk)：将输入张量切割为特定数量的块\n",
    "- [torch.matmul](https://pytorch.org/docs/stable/generated/torch.matmul.html?highlight=torch+matmul#torch.matmul)：执行两个张量之间的矩阵乘法\n",
    "- [torch.transpose](https://pytorch.org/docs/stable/generated/torch.transpose.html?highlight=torch+transpose#torch.transpose)：返回一个输入张量的转置，即互换两个维度\n",
    "- [torch.full](https://pytorch.org/docs/stable/generated/torch.transpose.html?highlight=torch+full#torch.full)：返回一个形状为 size，填充值为 fill_value 的张量\n",
    "- [torch.triu](https://pytorch.org/docs/stable/generated/torch.transpose.html?highlight=torch+triu#torch.triu)：返回一个张量，其中包含了输入矩阵的上三角部分，其他位置设为0\n",
    "\n",
    "## torch.nn.functional\n",
    "- [torch.nn.functional.softmax](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html?highlight=softmax#torch.nn.functional.softmax)：对输入数据进行 softmax 操作，这是一个常用于将实数向量规范化为概率分布的操作"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
